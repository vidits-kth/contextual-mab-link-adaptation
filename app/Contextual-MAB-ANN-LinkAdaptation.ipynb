{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import ray\n",
    "import time\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src import utils\n",
    "from src.link_adaptation_agents import MultiArmedBandit as MAB_agent\n",
    "from src.link_adaptation_agents import OuterLoopLinkAdaptation as OLLA_agent\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True, threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_model_datafile = 'Offline_Link_Model.npy'\n",
    "offline_model = np.load( offline_model_datafile, encoding='latin1', allow_pickle=True )[ ( ) ]\n",
    "\n",
    "snr_vs_bler = np.array(offline_model['snr_vs_bler'])\n",
    "snr_range_dB = np.array(offline_model['snr_range_dB'])\n",
    "\n",
    "nrof_snr, nrof_mcs = snr_vs_bler.shape\n",
    "\n",
    "plt.figure()\n",
    "plt.grid(True)\n",
    "for i in range(nrof_mcs):\n",
    "    plt.semilogy(snr_range_dB, snr_vs_bler[:, i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modorder = offline_model[\"modulation_order\"]\n",
    "tbs = offline_model[\"block_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for handling lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten( list_ ):\n",
    "    return [ item for sublist in list_ for item in sublist ]\n",
    "\n",
    "def stack( list_, sublist_size ):\n",
    "    nrof_sublists = int( len( list_ ) / sublist_size ) \n",
    "    return [ list_[ i * sublist_size : ( i + 1 ) * sublist_size ] for i in range( nrof_sublists ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap environment as Ray actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote( num_cpus = 0 )\n",
    "class myenv( ):\n",
    "    def __init__( self, env_name, nrof_instances ):\n",
    "        import gym\n",
    "        import gym_radio_link\n",
    "        self.envs = [ gym.make( env_name ) for _ in range( nrof_instances ) ]\n",
    "        \n",
    "    def seed( self, seed_values ):\n",
    "        return [ env.seed( seed ) for env, seed in zip( self.envs, seed_values ) ]\n",
    "    \n",
    "    def step( self, actions ):\n",
    "        return [ env.step(action) for env, action in zip( self.envs, actions ) ]\n",
    "    \n",
    "    def reset( self ):\n",
    "        return [ env.reset( ) for env in self.envs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "env_name = 'MCSSelection-v0'\n",
    "nrof_actions = nrof_mcs\n",
    "nrof_tti = 100\n",
    "cqi_delay = 0\n",
    "\n",
    "nrof_actors = 8\n",
    "nrof_links_per_actor = 4 * 8\n",
    "\n",
    "nrof_iter = int( 2 ** 14 / ( nrof_actors * nrof_links_per_actor ) )\n",
    "\n",
    "nrof_links = nrof_actors * nrof_links_per_actor\n",
    "\n",
    "# Reinforcement learning parameters\n",
    "obs_window = cqi_delay + 1\n",
    "learning_rate = 5e-4\n",
    "ann_layout = [ 16, 8, 8, 16 ]\n",
    "\n",
    "minibatch_size = min(16, nrof_links)\n",
    "sgd_iter = 8\n",
    "\n",
    "# Simulation parameters\n",
    "bler_target = 1.3\n",
    "\n",
    "olla_bler_target = 0.1\n",
    "olla_step_size = 0.1\n",
    "\n",
    "print(nrof_iter)\n",
    "print(nrof_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect parameters and create sim directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { 'env_name': env_name,\n",
    "               'obs_window': obs_window,\n",
    "               'nrof_tti': nrof_tti,\n",
    "               'cqi_delay': cqi_delay,\n",
    "               'nrof_links': nrof_links,\n",
    "               'bler_target': bler_target,\n",
    "               'learning_rate': learning_rate,\n",
    "               'ann_layout': ann_layout,\n",
    "               'minibatch_size': minibatch_size,\n",
    "               'sgd_iter': sgd_iter,\n",
    "               'olla_bler_target': olla_bler_target,\n",
    "               'olla_step_size': olla_step_size,\n",
    "               'nrof_iter': nrof_iter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dir = utils.create_new_simulation_directory( base_dir='../final_results' )\n",
    "\n",
    "print( 'Sim directory: ' + sim_dir )\n",
    "\n",
    "np.save( sim_dir + '/simulation_parameters.npy', parameters )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_redis_address = \"0.0.0.0:0000\" # Supply the address of the Redis server for your local Ray cluster.\n",
    "\n",
    "ray.init(redis_address = ray_redis_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def f( ):\n",
    "    time.sleep( 0.01 )\n",
    "    return ray.services.get_node_ip_address( )\n",
    "\n",
    "# Get a list of the IP addresses of the nodes that have joined the cluster.\n",
    "set( ray.get( [ f.remote( ) for _ in range( 100 ) ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_agent_statistics   = { 'tti': [ ], 'obs': [ ], 'actions': [ ], 'result': [ ], 'loss': [ ] }\n",
    "olla_agent_statistics = { 'tti': [ ], 'obs': [ ], 'actions': [ ], 'result': [ ] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning function in a thead to allow concurrency with actors\n",
    "train_lock = threading.Lock()\n",
    "def train_agent(obs, actions, rewards):\n",
    "    tic = time.time()\n",
    "    \n",
    "    indices = np.arange(len(obs))\n",
    "    for _ in range(sgd_iter):\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        shuffled_obs = obs[indices]\n",
    "        shuffled_actions = actions[indices]\n",
    "        shuffled_rewards = rewards[indices]\n",
    "        \n",
    "        for i in range(int(len(obs) / minibatch_size)):\n",
    "            obs_mbatch = shuffled_obs[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "            actions_mbatch = shuffled_actions[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "            rewards_mbatch = shuffled_rewards[i * minibatch_size : (i + 1) * minibatch_size]\n",
    "\n",
    "            rl_agent.update_agent(obs_mbatch, actions_mbatch, rewards_mbatch)\n",
    "            # print('Training time %0.4f'%(time.time() - tic))\n",
    "        \n",
    "    train_lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote( num_cpus = 0 )\n",
    "class olla_agent( ):\n",
    "    def __init__( self, bler_target, window_size, awgn_data, nrof_instances, olla_step_size ):\n",
    "        self.agents = [ OLLA_agent( bler_target, window_size, awgn_data, olla_step_size ) for _ in range( nrof_instances ) ]\n",
    "\n",
    "    def reset_agent( self ):\n",
    "        return [ agent.reset( ) for agent in self.agents ]\n",
    "        \n",
    "    def determine_action_indices( self, observations ):\n",
    "        return [ agent.determine_action_indices( obs ) for agent, obs in zip( self.agents, observations ) ]\n",
    "    \n",
    "    def determine_predicted_success( self, observations ):\n",
    "        return [ agent.determine_predicted_success( obs ) for agent, obs in zip( self.agents, observations ) ]\n",
    "    \n",
    "    def update_agent( self, rewards ):\n",
    "        return [ agent.update_agent( reward ) for agent, reward in zip( self.agents, rewards ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if olla_bler_target is not None:\n",
    "    link_bler_target = olla_bler_target * np.ones((nrof_links))\n",
    "else:\n",
    "    # Analytical BLER target from the following paper:\n",
    "    # Park, Sungwoo, Robert C. Daniels, and Robert W. Heath. \n",
    "    # \"Optimizing the target error rate for link adaptation.\" \n",
    "    # In 2015 IEEE Global Communications Conference (GLOBECOM), pp. 1-6. IEEE, 2015.\n",
    "    link_bler_target = 0.3 * np.exp(-0.08 * np.array(ue_snrs))\n",
    "\n",
    "# OLLA performance\n",
    "olla_agent_actors = [ olla_agent.remote( link_bler_target[i],\n",
    "                                         obs_window, \n",
    "                                         offline_model, \n",
    "                                         nrof_links_per_actor,\n",
    "                                         olla_step_size) for i in range( nrof_actors ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize environment Ray actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for expriment_iter in range(nrof_iter):\n",
    "\n",
    "    print('------------------------------------')\n",
    "    print('Experiment Iter: %d'%(expriment_iter))\n",
    "    print('------------------------------------')\n",
    "    \n",
    "    # Create actors which are remote instances of the environment\n",
    "    radio_link_actors = [ myenv.remote( env_name, nrof_links_per_actor ) for _ in range( nrof_actors ) ]\n",
    "\n",
    "    seed_offset = expriment_iter * nrof_links\n",
    "    generate_seeds = lambda i: np.arange( seed_offset + i * nrof_links_per_actor, seed_offset + ( i + 1 ) * nrof_links_per_actor, dtype = np.uint32 )\n",
    "\n",
    "    ue_params = flatten(ray.get( [ radio_link_actors[ i ].seed.remote( generate_seeds ( i ) ) for i in range( nrof_actors ) ] ))\n",
    "\n",
    "    ue_snrs = [p[0] for p in ue_params]\n",
    "    ue_speeds = [p[1] for p in ue_params]\n",
    "\n",
    "    #print(ue_snrs)\n",
    "    #print(ue_speeds)\n",
    "    \n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    # Contextual MAB\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    \n",
    "    # Create an instance of the RL agent\n",
    "    rl_agent = MAB_agent( 3, nrof_mcs, ann_layout, learning_rate )\n",
    "    \n",
    "    # Get the initial observation of the environment\n",
    "    obs = np.ones( ( nrof_links, obs_window ), dtype=np.int32 )\n",
    "\n",
    "    initial_obs = flatten( ray.get( [ actor.reset.remote( ) for actor in radio_link_actors ] ) )\n",
    "    obs[ :, -1 ] = initial_obs\n",
    "\n",
    "    # ----------------- Run experiment --------------------\n",
    "    start = time.time()\n",
    "    for step_index in range(nrof_tti + obs_window + 1):\n",
    "\n",
    "        # --------------- Prepare agent input -----------------\n",
    "        \n",
    "        agent_input = np.transpose(np.vstack((obs[:,0], ue_snrs, ue_speeds)))\n",
    "\n",
    "        # ------------------ Take action ----------------------\n",
    "        \n",
    "        actions  = []\n",
    "\n",
    "        predicted_success = rl_agent.calculate_output(agent_input)\n",
    "\n",
    "        for psuccess in predicted_success:\n",
    "            valid_mcs = np.array(psuccess > (1.0 - bler_target))\n",
    "            candidate_tputs = tbs * psuccess * valid_mcs.astype(np.float)\n",
    "            actions.append(np.argmax(candidate_tputs))\n",
    "\n",
    "        stacked_actions = stack( actions, nrof_links_per_actor )\n",
    "\n",
    "        # Take the specified action for each actor\n",
    "        # Returns the reward and the next observed state of the environment\n",
    "        tic = time.time()\n",
    "        result = flatten( ray.get([actor.step.remote(action) for actor, action in zip( radio_link_actors, \n",
    "                                                                                       stacked_actions ) ] ) )\n",
    "        # print('Step time %0.4f'%(time.time() - tic))\n",
    "\n",
    "        # ---------------- Collect performance stats------------\n",
    "        \n",
    "        if step_index >= cqi_delay:\n",
    "            if expriment_iter == 0:\n",
    "                rl_agent_statistics['tti'].append(step_index - cqi_delay)\n",
    "                rl_agent_statistics['result'].append(result)\n",
    "            else:\n",
    "                index = step_index - cqi_delay\n",
    "                rl_agent_statistics['result'][index] += result\n",
    "        \n",
    "        # ------------------ Update agent -----------------------\n",
    "        # Extact the reward\n",
    "        rewards = [int(result[i][3]) for i in range(nrof_links)]\n",
    "\n",
    "        # Update the RL agent after sufficient observations are available    \n",
    "        if step_index >= obs_window:\n",
    "            train_lock.acquire()\n",
    "            t = threading.Thread(target=train_agent, args = (agent_input, \n",
    "                                                             np.array(actions), \n",
    "                                                             np.array(rewards) ) )\n",
    "            t.daemon = True\n",
    "            t.start()\n",
    "\n",
    "\n",
    "        tputs = [int(result[i][1]) for i in range(nrof_links)]\n",
    "\n",
    "        loss = rl_agent.calculate_loss(agent_input, actions, rewards)\n",
    "\n",
    "        # ------------------ Update state -----------------------\n",
    "        obs[:,:-1] = obs[:,1:]\n",
    "        obs[:,-1] = [result[i][2] for i in range(nrof_links)]\n",
    "\n",
    "        \n",
    "        # ----------------- Print statistics --------------------\n",
    "        # Print statistics periodically\n",
    "        if step_index % (nrof_tti / 10) == 0:    \n",
    "            print('Step: %d, Mean Loss: %0.4f, BLER %0.2f, Tput: %d, Time: %ds'%(step_index, \n",
    "                                                                                 sum(loss) / len(loss), \n",
    "                                                                                 1.0 - sum(rewards) / len(rewards),\n",
    "                                                                                 sum(tputs) / len(tputs),\n",
    "                                                                                 time.time() - start))\n",
    "\n",
    "    print('Duration: %d s'%(time.time() - start))\n",
    "    \n",
    "    \n",
    "    if olla_bler_target is not None:\n",
    "        link_bler_target = olla_bler_target * np.ones((nrof_links))#* np.exp(-0.08 * np.array(ue_snrs))\n",
    "    else:\n",
    "        link_bler_target = 0.3 * np.exp(-0.08 * np.array(ue_snrs))\n",
    "\n",
    "    # OLLA performance\n",
    "    olla_agent_actors = [ olla_agent.remote( link_bler_target[i], #0.3 * np.exp(-0.08 * ue_snrs[i]), \n",
    "                                             obs_window, \n",
    "                                             offline_model, \n",
    "                                             nrof_links_per_actor,\n",
    "                                             olla_step_size) for i in range( nrof_actors ) ]\n",
    "\n",
    "\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    # Outer Loop Link Adaptation (OLLA)\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    #-------------------------\n",
    "    \n",
    "    obs = np.ones( ( nrof_links, obs_window ), dtype=np.int32 )\n",
    "\n",
    "    # Reset the environment\n",
    "    obs[:,-1] = flatten(ray.get( [ actor.reset.remote( ) for actor in radio_link_actors ] ))\n",
    "\n",
    "    # Reset the OLLA actor\n",
    "    _ = ray.get([olla_agent_actors[i].reset_agent.remote() for i in range(nrof_actors)])\n",
    "\n",
    "    start = time.time( )\n",
    "    for step_index in range( nrof_tti + obs_window + 1 ):\n",
    "\n",
    "        # --------------- Prepare agent input -----------------\n",
    "        latest_obs = stack(obs[:,0], nrof_links_per_actor)\n",
    "\n",
    "        # ------------------ Take action ----------------------\n",
    "        \n",
    "        predicted_success = flatten(ray.get([olla_agent_actors[i].determine_predicted_success.remote(latest_obs[i]) for i in range(nrof_actors)]))\n",
    "\n",
    "        actions  = []\n",
    "\n",
    "        for link_index, psuccess in enumerate(predicted_success):\n",
    "            valid_mcs = np.array(psuccess > (1.0 - link_bler_target[link_index]))\n",
    "            candidate_tputs = tbs * psuccess * valid_mcs.astype(np.float)\n",
    "            actions.append(np.argmax(candidate_tputs))\n",
    "            \n",
    "        actions = stack( actions, nrof_links_per_actor )\n",
    "\n",
    "        # Take the OLLA-guided action for each actor\n",
    "        # Returns the reward and the next observed state of the environment\n",
    "        result = flatten( ray.get([actor.step.remote(action) for actor, action in zip( radio_link_actors, actions )]) )\n",
    "\n",
    "        # ---------------- Collect performance stats------------\n",
    "        # Collect the result statistics\n",
    "        if step_index >= cqi_delay:\n",
    "            if expriment_iter == 0:\n",
    "                # First experiment run\n",
    "                olla_agent_statistics['tti'].append(step_index - cqi_delay)\n",
    "                olla_agent_statistics['result'].append(result)\n",
    "            else:\n",
    "                # Subsequent experiment runs\n",
    "                index = step_index - cqi_delay\n",
    "                olla_agent_statistics['result'][index] += result\n",
    "\n",
    "        # ------------------ Update agent -----------------------\n",
    "        # Extact the reward function\n",
    "        acks = [int(result[i][3]) for i in range(nrof_links)]\n",
    "        tputs = [int(result[i][1]) for i in range(nrof_links)]\n",
    "                \n",
    "        # Start updating the agent after sufficient observations are available\n",
    "        if step_index >= obs_window:\n",
    "            acks_stacked = stack(acks, nrof_links_per_actor)\n",
    "            ray.get([olla_agent_actors[i].update_agent.remote(acks_stacked[i]) for i in range(nrof_actors)])\n",
    "        \n",
    "        # ------------------ Update state -----------------------\n",
    "        # Update the channel state\n",
    "        obs[:,:-1] = obs[:,1:]\n",
    "        obs[:,-1] = [result[i][2] for i in range(nrof_links)]\n",
    "\n",
    "        # ----------------- Print statistics --------------------\n",
    "        \n",
    "        # Print statistics periodically\n",
    "        if step_index % (nrof_tti / 10) == 0:\n",
    "            print('Step: %d, BLER: %0.2f, Tput: %d, Time: %ds'%(step_index, \n",
    "                                                                1.0 - sum(acks) / len(acks), \n",
    "                                                                sum(tputs) / len(tputs),\n",
    "                                                                time.time() - start))\n",
    "\n",
    "    print('Duration: %d s'%(time.time() - start))\n",
    "    \n",
    "# Save the results of the simulation\n",
    "np.save(sim_dir + '/mab_statistics.npy', rl_agent_statistics)\n",
    "np.save(sim_dir + '/olla_statistics.npy', olla_agent_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
